{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import threading\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Optional, Tuple, Union\n",
    "from uuid import uuid4\n",
    "\n",
    "import cv2\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import whisper\n",
    "from moviepy import editor\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "from pyAudioAnalysis import MidTermFeatures as mtf\n",
    "from pyAudioAnalysis import audioTrainTest as at\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from torchvision import models, transforms\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_videos_to = \"data\"\n",
    "window = 10\n",
    "summary_output = \"videos_summary\"\n",
    "num_highlights = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageHighlightsFinder:\n",
    "    def __init__(self, batch_size: int = 32) -> None:\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.batch_size = batch_size\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        self.model = torch.nn.Sequential(*(list(model.children())[:-1])).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def _get_transformations(self, will_be_saved: bool) -> List[Any]:\n",
    "        transformations = [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "        if will_be_saved:\n",
    "            transformations.append(transforms.ToPILImage())\n",
    "        return transformations\n",
    "\n",
    "    def _preprocess_image(\n",
    "        self, image: Image.Image, will_be_saved: bool = False\n",
    "    ) -> Union[torch.Tensor, Image.Image]:\n",
    "        transformations = self._get_transformations(will_be_saved=will_be_saved)\n",
    "\n",
    "        transform = transforms.Compose(transformations)\n",
    "        image = transform(image)\n",
    "        if will_be_saved:\n",
    "            return image\n",
    "\n",
    "        image = image.unsqueeze(0)\n",
    "        # print(image.shape)\n",
    "        return image\n",
    "\n",
    "    def _chunks(self, lst, n):\n",
    "        \"\"\"\n",
    "        Yield successive n-sized chunks from lst.\n",
    "        \"\"\"\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i : i + n]\n",
    "\n",
    "    def _create_feature_vectors(self, file_paths: List[str]) -> np.ndarray:\n",
    "        features = None\n",
    "        for file_paths_chunk in self._chunks(file_paths, n=self.batch_size):\n",
    "            # Get the data for this batch.\n",
    "            imgs = [Image.open(img).convert(\"RGB\") for img in file_paths_chunk]\n",
    "            imgs = [self._preprocess_image(img) for img in imgs]\n",
    "            imgs = torch.cat(imgs, dim=0).to(self.device)\n",
    "\n",
    "            # Convert them to features.\n",
    "            with torch.no_grad():\n",
    "                f = self.model(imgs)\n",
    "\n",
    "            if features is None:\n",
    "                features = f.clone()\n",
    "            else:\n",
    "                features = torch.cat((features, f), dim=0)\n",
    "\n",
    "        features = features.squeeze(dim=-1)\n",
    "        features = features.squeeze(dim=-1)\n",
    "        features = features.cpu().detach().numpy()\n",
    "        return features\n",
    "\n",
    "    def _extract_frames(\n",
    "        self,\n",
    "        video_path: str,\n",
    "        images_folder: str,\n",
    "        start_at_sec: int = 5,\n",
    "        window: int = 10,\n",
    "    ):\n",
    "        os.makedirs(images_folder, exist_ok=True)\n",
    "        duration = editor.VideoFileClip(video_path).duration\n",
    "        vidcap = cv2.VideoCapture(video_path)\n",
    "        success, image = vidcap.read()\n",
    "        success = True\n",
    "        while success & (start_at_sec <= duration):\n",
    "            vidcap.set(\n",
    "                cv2.CAP_PROP_POS_MSEC, (start_at_sec * 1000)\n",
    "            )  # One frame per second.\n",
    "            success, image = vidcap.read()\n",
    "            # print(\"Read a new frame: \", success)\n",
    "            if success:\n",
    "                cv2.imwrite(\n",
    "                    os.path.join(images_folder, f\"sec_{start_at_sec}.jpg\"), image\n",
    "                )  # save frame as JPEG file.\n",
    "            start_at_sec += window\n",
    "\n",
    "    def _get_ground_percentage(self, file_paths: List[str]) -> np.ndarray:\n",
    "        perc = []\n",
    "        for img_path in file_paths:\n",
    "            img = cv2.imread(img_path)\n",
    "            hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "            # Green range\n",
    "            lower_green = np.array([40,40, 40])\n",
    "            upper_green = np.array([70, 255, 255])\n",
    "\n",
    "            # Masking\n",
    "            mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "            res = cv2.bitwise_and(img, img, mask=mask)\n",
    "            res_gray = cv2.cvtColor(res,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Defining a kernel to do morphological operation to get better output.\n",
    "            kernel = np.ones((5,5),np.uint8)\n",
    "            thresh = cv2.threshold(res_gray,127,255,cv2.THRESH_BINARY_INV |  cv2.THRESH_OTSU)[1]\n",
    "            thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "            ground_percentage = (thresh.size - np.count_nonzero(thresh))/thresh.size\n",
    "\n",
    "            perc.append(ground_percentage)\n",
    "\n",
    "        return np.array(perc)\n",
    "\n",
    "    def get_distances(\n",
    "        self,\n",
    "        video_path: str,\n",
    "        temp_path: str,\n",
    "        window: int = 10,\n",
    "    ) -> np.ndarray:\n",
    "        temp_file_path = os.path.join(temp_path, \"image\")\n",
    "        if not os.path.exists(temp_file_path):\n",
    "            start_at_sec = int(window * 0.5)\n",
    "            self._extract_frames(\n",
    "                video_path=video_path,\n",
    "                images_folder=temp_file_path,\n",
    "                start_at_sec=start_at_sec,\n",
    "                window=window,\n",
    "            )\n",
    "        file_paths = glob.glob(os.path.join(temp_file_path, \"*.jpg\"))\n",
    "        image_features_file = os.path.join(temp_path, \"image_features.pkl\")\n",
    "        if os.path.exists(image_features_file):\n",
    "            with open(image_features_file,'rb') as f:\n",
    "                features = pickle.load(f)\n",
    "        else:\n",
    "            features = self._create_feature_vectors(file_paths=file_paths)\n",
    "            with open(image_features_file, 'wb') as f:\n",
    "                pickle.dump(features, f)\n",
    "        distances = cosine_distances(features, features)\n",
    "        del features\n",
    "        median_distances = np.median(distances, axis=1)\n",
    "        ground_perc = self._get_ground_percentage(file_paths=file_paths)\n",
    "        median_distances = np.where(ground_perc < 0.05, 0, median_distances)\n",
    "        del distances\n",
    "        assert median_distances.shape[0] == len(file_paths)\n",
    "        # shutil.rmtree(temp_file_path, ignore_errors=True)\n",
    "        return median_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioHighlightsFinder:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def _audio_seg(self, video_path: str, output_path: str, window: int = 10):\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        output_name = \"output_%04d.wav\"\n",
    "        subprocess.call(\n",
    "            [\n",
    "                \"ffmpeg\",\n",
    "                \"-y\",\n",
    "                \"-i\",\n",
    "                video_path,\n",
    "                \"-f\",\n",
    "                \"segment\",\n",
    "                \"-segment_time\",\n",
    "                str(window),\n",
    "                os.path.join(output_path, output_name),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Rename the segments\n",
    "        for seg_path in glob.glob(os.path.join(output_path, \"*wav\")):\n",
    "            audio_name = os.path.basename(seg_path)\n",
    "            audio_name = Path(audio_name).stem\n",
    "\n",
    "            audio_idx = audio_name.split(\"_\")[-1]\n",
    "            audio_idx = int(audio_idx)\n",
    "            # print(int(audio_idx))\n",
    "            start_ = audio_idx * window\n",
    "            end_ = (audio_idx + 1) * window\n",
    "            new_name = f\"sec_{int(start_)}_{int(end_)}\"\n",
    "\n",
    "            new_path = seg_path.replace(audio_name, new_name)\n",
    "\n",
    "            os.rename(seg_path, new_path)\n",
    "\n",
    "    def mean_amplitude(self, seg_path: str):\n",
    "        y, sr = librosa.load(seg_path)\n",
    "        second = []\n",
    "        for s in range(0, len(y), sr):\n",
    "            second.append(np.abs(y[s : s + sr]).mean())\n",
    "        return np.mean(second)\n",
    "\n",
    "    def _get_silent(self, segments_path: str, thres_q = 30) -> Tuple[List[int], List[str]]:\n",
    "        segs_amp = []\n",
    "        file_names = []\n",
    "        for seg_name in glob.glob(os.path.join(segments_path, \"*.wav\")):\n",
    "            seg_amplitude = self.mean_amplitude(seg_name)\n",
    "            segs_amp.append(seg_amplitude)\n",
    "            \n",
    "            file_names.append(seg_name)\n",
    "            \n",
    "        # threshold = np.mean(segs_amp)\n",
    "        threshold = np.percentile(segs_amp, q=thres_q)\n",
    "        is_silence = [1 if seg_amp < threshold else 0 for seg_amp in segs_amp]\n",
    "        return is_silence, file_names\n",
    "\n",
    "\n",
    "    def _feature_extraction(self, directory: str) -> Tuple[np.ndarray, List[str]]:\n",
    "        f1, _, feature_names = mtf.directory_feature_extraction(\n",
    "            directory, 1, 1, 0.1, 0.1\n",
    "        )\n",
    "        mid_term_features = [f1]\n",
    "        # convert list of feature matrices to x, y format:\n",
    "        x, y = at.features_to_matrix(mid_term_features)\n",
    "        m = x.mean(axis=0)\n",
    "        s = np.std(x, axis=0)\n",
    "        X = (x - m) / s\n",
    "        return X, feature_names\n",
    "\n",
    "    def _feature_selection(\n",
    "        self, X: np.ndarray, feature_names: List[str]\n",
    "    ) -> Tuple[np.ndarray, List[str]]:\n",
    "        # Choose Features that have some variability\n",
    "        threshold = 1\n",
    "        selector = VarianceThreshold(threshold=threshold)\n",
    "        X_selected = selector.fit_transform(X)\n",
    "        selected_feature_indices = selector.get_support(indices=True)\n",
    "        selected_feature_names = [feature_names[i] for i in selected_feature_indices]\n",
    "        return X_selected, selected_feature_names\n",
    "\n",
    "    def _outlier_detection(self, X: np.ndarray, num_high: int):\n",
    "        clf = LocalOutlierFactor(n_neighbors=20, metric=\"cosine\")\n",
    "        clf.fit(X)\n",
    "        outlier_scores = clf.negative_outlier_factor_\n",
    "        sorted_indices = np.argsort(outlier_scores)\n",
    "        highlight_indices = sorted_indices[:num_high]\n",
    "        # print(outlier_detection(X_new, 10))\n",
    "        return highlight_indices\n",
    "\n",
    "    def get_distances(\n",
    "        self,\n",
    "        video_path: str,\n",
    "        temp_path: str,\n",
    "        window: int = 10,\n",
    "    ) -> np.ndarray:\n",
    "        temp_file_path = os.path.join(temp_path, \"audio\")\n",
    "        # Cut the file into smaller chunks.\n",
    "        if not os.path.exists(temp_file_path):\n",
    "            self._audio_seg(\n",
    "                video_path=video_path,\n",
    "                output_path=temp_file_path,\n",
    "                window=window,\n",
    "            )\n",
    "\n",
    "        is_silence, file_names = self._get_silent(\n",
    "            segments_path=temp_file_path\n",
    "        )\n",
    "        audio_features_file = os.path.join(temp_path, \"audio_features.pkl\")\n",
    "        audio_feature_names_file = os.path.join(temp_path, \"audio_feature_names.pkl\")\n",
    "        if os.path.exists(audio_features_file):\n",
    "            with open(audio_features_file,'rb') as f:\n",
    "                X = pickle.load(f)\n",
    "            with open(audio_feature_names_file,'rb') as f:\n",
    "                feature_names = pickle.load(f)\n",
    "        else:\n",
    "            X, feature_names = self._feature_extraction(directory=temp_file_path)\n",
    "            with open(audio_features_file, 'wb') as f:\n",
    "                pickle.dump(X, f)\n",
    "            with open(audio_feature_names_file, 'wb') as f:\n",
    "                pickle.dump(feature_names, f)\n",
    "        \n",
    "        \n",
    "        X, feature_names = self._feature_selection(X=X, feature_names=feature_names)\n",
    "\n",
    "        distances = cosine_distances(X, X)\n",
    "        del X\n",
    "        median_distances = np.median(distances, axis=1)\n",
    "        # Make distance equal to 0 for the silences to be ignored.\n",
    "        median_distances = np.where(np.array(is_silence) == 1, 0, median_distances)\n",
    "\n",
    "        # shutil.rmtree(temp_file_path, ignore_errors=True)\n",
    "        return median_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextHighlightsFinder:\n",
    "    def __init__(self) -> None:\n",
    "        self.ahf = AudioHighlightsFinder()\n",
    "\n",
    "    def _transcript_thread_callback(self, filepath_list, model, output_path):\n",
    "        for file in filepath_list:\n",
    "            filename = file.split('\\\\')[-1].replace('wav','txt')\n",
    "            filepath = os.path.join(output_path, filename)\n",
    "            f = open(filepath, 'w')\n",
    "            try:\n",
    "                f.write(model.transcribe(file)['text'])\n",
    "            except:\n",
    "                f.write('')\n",
    "            f.close()\n",
    "\n",
    "    def transcript_clips(self, non_silent_files: List[str], output_path: str) -> None:\n",
    "\n",
    "        if len(non_silent_files) >= 8:\n",
    "            batch_size = len(non_silent_files)//8\n",
    "        else:\n",
    "            batch_size = len(non_silent_files)\n",
    "        \n",
    "        chunks = [non_silent_files[i:(i+batch_size)] for i in range(0, len(non_silent_files), batch_size)]\n",
    "\n",
    "        t = []\n",
    "        for i in range(0, len(chunks)):\n",
    "            sm_model_whisper = whisper.load_model(\"small.en\")\n",
    "            t.append(threading.Thread(target=self._transcript_thread_callback, args=(chunks[i], sm_model_whisper, output_path)))\n",
    "            t[i].start()\n",
    "\n",
    "        for i in t:\n",
    "            i.join()\n",
    "\n",
    "    def _silent_files_filler(self, silent_files, output_path):\n",
    "        for file in silent_files:\n",
    "            filename = file.split('\\\\')[-1].replace('wav','txt')\n",
    "            filepath = os.path.join(output_path, filename)\n",
    "            with open(filepath, 'w') as f:\n",
    "                f.write(\"\")\n",
    "\n",
    "    def _feature_extraction(self, directory: str) -> Tuple[np.ndarray, List[str]]:\n",
    "        texts = []\n",
    "        for text_file in os.listdir(directory):\n",
    "            with open(os.path.join(directory, text_file), 'r') as f:\n",
    "                texts.append(f.read())\n",
    "            \n",
    "        classifier = pipeline(\"sentiment-analysis\", model=\"michellejieli/emotion_text_classifier\", top_k=None)\n",
    "        results  = classifier(texts)\n",
    "        results = [sorted([tuple(i.values()) for i in j]) for j in results]\n",
    "\n",
    "        scores = [[j[1] for j in i] for i in results]\n",
    "        feature_names = [j[0] for j in results[0]]\n",
    "\n",
    "        speech_rate = [len(t.strip().split(' ')) for t in texts]\n",
    "        scores = np.array([i[0] + [i[1]] for i in list(zip(scores, speech_rate))])\n",
    "\n",
    "        feature_names.append('speech_rate')\n",
    "\n",
    "        return scores, feature_names\n",
    "\n",
    "    def _calculate_score(self, x):\n",
    "\n",
    "        m = x.mean(axis=0)\n",
    "        s = np.std(x, axis=0)\n",
    "        X = (x - m) / s\n",
    "\n",
    "        distances = cosine_distances(X, X)\n",
    "        median_distances = np.median(distances, axis=1)\n",
    "\n",
    "        w_score = 0.6*median_distances + 0.2*X[:,3] + 0.2*X[:,6]\n",
    "\n",
    "        return w_score\n",
    "\n",
    "\n",
    "    def get_distances(\n",
    "        self,\n",
    "        temp_path: str,\n",
    "    ) -> np.ndarray:\n",
    "        \n",
    "        temp_file_path = os.path.join(temp_path, \"text\")\n",
    "        if not os.path.exists(temp_file_path):\n",
    "            os.makedirs(temp_file_path, exist_ok=True)\n",
    "            temp_file_audio_path = os.path.join(temp_path, \"audio\")\n",
    "            is_silent, files = self.ahf._get_silent(temp_file_audio_path, thres_q=80)\n",
    "            non_silent_files = [i[1] for i in list(zip(is_silent, files)) if i[0]==0]\n",
    "            self.transcript_clips(non_silent_files, temp_file_path)\n",
    "            silent_files = [i[1] for i in list(zip(is_silent, files)) if i[0]==1]\n",
    "            self._silent_files_filler(silent_files, temp_file_path)\n",
    "        \n",
    "        x, feature_names = self._feature_extraction(temp_file_path)\n",
    "\n",
    "        w_score = self._calculate_score(x)\n",
    "\n",
    "        # shutil.rmtree(temp_file_path, ignore_errors=True)\n",
    "        return w_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighlightsFinder:\n",
    "    def __init__(self, batch_size: int = 32) -> None:\n",
    "        self.ahf = AudioHighlightsFinder()\n",
    "        self.thf = TextHighlightsFinder()\n",
    "        self.ihf = ImageHighlightsFinder(batch_size=batch_size)\n",
    "    \n",
    "    def _str_to_int_tuple(self, s: str) -> Tuple[int, int]:\n",
    "        start, end = s.split(\"_\")\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        return start, end\n",
    "\n",
    "    def _merge_timestamps(self, timestamps: List[Tuple]) -> List[Tuple]:\n",
    "        merged_timestamps = []\n",
    "        timestamps.sort(key=lambda x: x[0])  # Sort the timestamps based on start time\n",
    "\n",
    "        for timestamp in timestamps:\n",
    "            if merged_timestamps and timestamp[0] == merged_timestamps[-1][1]:\n",
    "                merged_timestamps[-1] = (\n",
    "                    merged_timestamps[-1][0],\n",
    "                    timestamp[1],\n",
    "                )  # Extend the previous timestamp\n",
    "            else:\n",
    "                merged_timestamps.append(timestamp)  # Add a new timestamp\n",
    "\n",
    "        return merged_timestamps\n",
    "\n",
    "    def _convert_str_to_timestamps(self, highlights: List[str]) -> List[Tuple]:\n",
    "        timestamps = [\n",
    "            self._str_to_int_tuple(s=timestamp)\n",
    "            for timestamp in highlights\n",
    "        ]\n",
    "        timestamps = self._merge_timestamps(timestamps=timestamps)\n",
    "        return timestamps\n",
    "\n",
    "    def create_video_summary(\n",
    "        self, video_path: str, summary_output: str, num_highlights: int, window: int\n",
    "    ):\n",
    "        # Get video's name.\n",
    "        video_name = os.path.basename(video_path)\n",
    "        video_name = Path(video_name).stem\n",
    "\n",
    "        temp_file_dir = \"temp\"\n",
    "        temp_file_path = os.path.join(temp_file_dir, video_name)\n",
    "\n",
    "        # Get the distances from each modality.\n",
    "        image_distances = self.ihf.get_distances(\n",
    "            video_path=video_path, temp_path=temp_file_path, window=window\n",
    "        )\n",
    "        audio_distances = self.ahf.get_distances(\n",
    "            video_path=video_path, temp_path=temp_file_path, window=window\n",
    "        )\n",
    "        text_distances = self.thf.get_distances(\n",
    "            temp_path=temp_file_path\n",
    "        )\n",
    "        # This means we have the last audio which is smaller than window.\n",
    "        if image_distances.shape[0] != audio_distances.shape[0]:\n",
    "            audio_distances = audio_distances[:-1].copy()\n",
    "            text_distances = text_distances[:-1].copy()\n",
    "            \n",
    "        # assert image_distances.shape[0] == audio_distances.shape[0]\n",
    "\n",
    "        # TODO: Add weight to each modality.\n",
    "\n",
    "        image_distances = (image_distances-np.min(image_distances))/(np.max(image_distances)-np.min(image_distances))\n",
    "        audio_distances = (audio_distances-np.min(audio_distances))/(np.max(audio_distances)-np.min(audio_distances))\n",
    "        text_distances = (text_distances-np.min(text_distances))/(np.max(text_distances)-np.min(text_distances))\n",
    "\n",
    "        distances = np.add(image_distances, audio_distances, text_distances)\n",
    "\n",
    "        # Get the idx of the segments with the greater distance.\n",
    "        idx = np.argsort(distances)[-num_highlights:]\n",
    "\n",
    "        # Load the video.\n",
    "        video = editor.VideoFileClip(video_path)\n",
    "        # Get the duration of the video in secs.\n",
    "        duration = video.duration\n",
    "\n",
    "        # Create the timestamps in the same way as they will get processed.\n",
    "        timestamps = [\n",
    "            f\"{(j - 1) * window}_{j * window}\"\n",
    "            for j in range(1, math.floor(duration / window) + 1)\n",
    "        ]\n",
    "        timestamps = list(sorted(timestamps))\n",
    "\n",
    "        # Get the timestamps of the segments with the greater distance.\n",
    "        highlights = np.array(timestamps)[idx].tolist()\n",
    "\n",
    "        timestamps = self._convert_str_to_timestamps(highlights=highlights)\n",
    "        \n",
    "        # Create the summary video.\n",
    "        clips = []\n",
    "        for start_time , end_time in timestamps:\n",
    "            clip = video.subclip(start_time, end_time)\n",
    "            clips.append(clip)\n",
    "        \n",
    "        final = editor.concatenate_videoclips(clips)\n",
    "        \n",
    "        os.makedirs(summary_output, exist_ok=True)\n",
    "        final.write_videofile(os.path.join(summary_output, f\"{video_name}_summary.mp4\"))\n",
    "        # Delete the temp dir.\n",
    "        # shutil.rmtree(temp_file_dir, ignore_errors=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Download Videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=SvV6aUki6LU&list=PLCGIzmTE4d0iCqSmha1X7F-_AqB3jjo26&index=7&ab_channel=FIFA\n",
      "[youtube:tab] Downloading just the video SvV6aUki6LU because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=SvV6aUki6LU\n",
      "[youtube] SvV6aUki6LU: Downloading webpage\n",
      "[youtube] SvV6aUki6LU: Downloading ios player API JSON\n",
      "[youtube] SvV6aUki6LU: Downloading android player API JSON\n",
      "[youtube] SvV6aUki6LU: Downloading m3u8 information\n",
      "[info] SvV6aUki6LU: Downloading 1 format(s): 614+140\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 1207\n",
      "[download] Destination: data\\SvV6aUki6LU.f614.mp4\n",
      "[download] 100% of    1.70GiB in 00:04:57 at 5.84MiB/s                        \n",
      "[download] Destination: data\\SvV6aUki6LU.f140.m4a\n",
      "[download] 100% of   94.38MiB in 00:00:08 at 10.59MiB/s    \n",
      "[Merger] Merging formats into \"data\\SvV6aUki6LU.mp4\"\n",
      "Deleting original file data\\SvV6aUki6LU.f140.m4a (pass -k to keep)\n",
      "Deleting original file data\\SvV6aUki6LU.f614.mp4 (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "links = [\n",
    "    \"https://www.youtube.com/watch?v=SvV6aUki6LU&list=PLCGIzmTE4d0iCqSmha1X7F-_AqB3jjo26&index=6&ab_channel=FIFA\",\n",
    "    \"https://www.youtube.com/watch?v=oZEVgDXJwCc&list=PLCGIzmTE4d0iCqSmha1X7F-_AqB3jjo26&index=7&ab_channel=FIFA\",\n",
    "    \"https://www.youtube.com/watch?v=FopM2tiNJO4&list=PLCGIzmTE4d0iCqSmha1X7F-_AqB3jjo26&index=10&ab_channel=FIFA\",\n",
    "    \"https://www.youtube.com/watch?v=WlNAln9mcg8&list=PLCGIzmTE4d0iCqSmha1X7F-_AqB3jjo26&index=11&ab_channel=FIFA\",\n",
    "    \"https://www.youtube.com/watch?v=xPfs2JL_4ws&list=PLCGIzmTE4d0iCqSmha1X7F-_AqB3jjo26&index=12&ab_channel=FIFA\",\n",
    "    \"https://www.youtube.com/watch?v=Cbij3MKhdOY&list=PLCGIzmTE4d0iCqSmha1X7F-_AqB3jjo26&index=14&ab_channel=FIFA\",\n",
    "    \"https://www.youtube.com/watch?v=L6sbfskaTDQ&list=PLCGIzmTE4d0iCqSmha1X7F-_AqB3jjo26&index=16&ab_channel=FIFA\",\n",
    "    \"https://www.youtube.com/watch?v=i6DaUHROjTg&list=PLCGIzmTE4d0iCqSmha1X7F-_AqB3jjo26&index=3&ab_channel=FIFA\",\n",
    "    \"https://www.youtube.com/watch?v=b-HZviMbqxc&list=PLCGIzmTE4d0iCqSmha1X7F-_AqB3jjo26&index=2&ab_channel=FIFA\",\n",
    "    \"https://www.youtube.com/watch?v=SirRnkDOrlU&list=PLCGIzmTE4d0iCqSmha1X7F-_AqB3jjo26&index=5&ab_channel=FIFA\"\n",
    "]\n",
    "\n",
    "ydl_opts = {\"noplaylist\": True, \"outtmpl\": os.path.join(save_videos_to, \"%(id)s\"), \"format\": \"bv*[ext=mp4]+ba[ext=m4a]/b[ext=mp4] / bv*+ba/b\"}\n",
    "\n",
    "with YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download(links)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create Video Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = HighlightsFinder(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_output_path = [f for f in glob.glob(os.path.join(save_videos_to, \"*.mp4\")) if \"_summary\" not in f and \"ipynb_checkpoints\" not in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video videos_summary\\SvV6aUki6LU_summary.mp4.\n",
      "MoviePy - Writing audio in SvV6aUki6LU_summaryTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video videos_summary\\SvV6aUki6LU_summary.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos_summary\\SvV6aUki6LU_summary.mp4\n"
     ]
    }
   ],
   "source": [
    "for video_path in videos_output_path:\n",
    "    hf.create_video_summary(video_path=video_path, summary_output=summary_output, num_highlights=num_highlights, window=window)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries & Pre-trained Models References\n",
    "He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).\n",
    "\n",
    "Jochen Hartmann, \"Emotion English DistilRoBERTa-base\". https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/, 2022.\n",
    "Ashritha R Murthy and K M Anil Kumar 2021 IOP Conf. Ser.: Mater. Sci. Eng. 1110 012009\n",
    "\n",
    "Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2022). Robust speech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356.\n",
    "\n",
    "Giannakopoulos, T. (2015). pyaudioanalysis: An open-source python library for audio signal analysis. PloS one, 10(12), e0144610."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videosummarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
