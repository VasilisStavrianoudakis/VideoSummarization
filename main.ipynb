{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "\n",
    "# warnings.simplefilter(action='ignore')\n",
    "import os\n",
    "import shutil\n",
    "# import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Optional, Tuple, Union\n",
    "from uuid import uuid4\n",
    "\n",
    "# import av\n",
    "import cv2\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from moviepy import editor\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "from PIL import Image\n",
    "from pyAudioAnalysis import MidTermFeatures as mtf\n",
    "from pyAudioAnalysis import audioTrainTest as at\n",
    "from pydub import AudioSegment, silence\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from torchvision import models, transforms\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "# from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_videos_to = \"data\"\n",
    "window = 10\n",
    "summary_output = \"videos_summary\"\n",
    "num_highlights = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageHighlightsFinder:\n",
    "    def __init__(self, batch_size: int = 32) -> None:\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.batch_size = batch_size\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        self.model = torch.nn.Sequential(*(list(model.children())[:-1])).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def _get_transformations(self, will_be_saved: bool) -> List[Any]:\n",
    "        transformations = [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "        if will_be_saved:\n",
    "            transformations.append(transforms.ToPILImage())\n",
    "        return transformations\n",
    "\n",
    "    def _preprocess_image(\n",
    "        self, image: Image.Image, will_be_saved: bool = False\n",
    "    ) -> Union[torch.Tensor, Image.Image]:\n",
    "        transformations = self._get_transformations(will_be_saved=will_be_saved)\n",
    "\n",
    "        transform = transforms.Compose(transformations)\n",
    "        image = transform(image)\n",
    "        if will_be_saved:\n",
    "            return image\n",
    "\n",
    "        image = image.unsqueeze(0)\n",
    "        # print(image.shape)\n",
    "        return image\n",
    "\n",
    "    def _chunks(self, lst, n):\n",
    "        \"\"\"\n",
    "        Yield successive n-sized chunks from lst.\n",
    "        \"\"\"\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i : i + n]\n",
    "\n",
    "    def _create_feature_vectors(self, file_paths: List[str]) -> np.ndarray:\n",
    "        features = None\n",
    "        for file_paths_chunk in self._chunks(file_paths, n=self.batch_size):\n",
    "            # Get the data for this batch.\n",
    "            imgs = [Image.open(img).convert(\"RGB\") for img in file_paths_chunk]\n",
    "            imgs = [self._preprocess_image(img) for img in imgs]\n",
    "            imgs = torch.cat(imgs, dim=0).to(self.device)\n",
    "\n",
    "            # Convert them to features.\n",
    "            with torch.no_grad():\n",
    "                f = self.model(imgs)\n",
    "\n",
    "            if features is None:\n",
    "                features = f.clone()\n",
    "            else:\n",
    "                features = torch.cat((features, f), dim=0)\n",
    "\n",
    "        features = features.squeeze(dim=-1)\n",
    "        features = features.squeeze(dim=-1)\n",
    "        features = features.cpu().detach().numpy()\n",
    "        return features\n",
    "\n",
    "    def _extract_frames(\n",
    "        self,\n",
    "        video_path: str,\n",
    "        images_folder: str,\n",
    "        start_at_sec: int = 5,\n",
    "        window: int = 10,\n",
    "    ):\n",
    "        os.makedirs(images_folder, exist_ok=True)\n",
    "        vidcap = cv2.VideoCapture(video_path)\n",
    "        success, image = vidcap.read()\n",
    "        success = True\n",
    "        while success:\n",
    "            vidcap.set(\n",
    "                cv2.CAP_PROP_POS_MSEC, (start_at_sec * 1000)\n",
    "            )  # One frame per second.\n",
    "            success, image = vidcap.read()\n",
    "            # print(\"Read a new frame: \", success)\n",
    "            if success:\n",
    "                cv2.imwrite(\n",
    "                    os.path.join(images_folder, f\"sec_{start_at_sec}.jpg\"), image\n",
    "                )  # save frame as JPEG file.\n",
    "            start_at_sec += window\n",
    "\n",
    "    def get_distances(\n",
    "        self,\n",
    "        video_path: str,\n",
    "        temp_path: str,\n",
    "        window: int = 10,\n",
    "    ) -> np.ndarray:\n",
    "        temp_file_path = os.path.join(temp_path, \"image\")\n",
    "        start_at_sec = int(window * 0.5)\n",
    "        self._extract_frames(\n",
    "            video_path=video_path,\n",
    "            images_folder=temp_file_path,\n",
    "            start_at_sec=start_at_sec,\n",
    "            window=window,\n",
    "        )\n",
    "        file_paths = glob.glob(os.path.join(temp_file_path, \"*.jpg\"))\n",
    "        features = self._create_feature_vectors(file_paths=file_paths)\n",
    "        distances = cosine_distances(features, features)\n",
    "        del features\n",
    "        median_distances = np.median(distances, axis=1)\n",
    "        del distances\n",
    "        assert median_distances.shape[0] == len(file_paths)\n",
    "        shutil.rmtree(temp_file_path, ignore_errors=True)\n",
    "        return median_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioHighlightsFinder:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def _audio_seg(\n",
    "        self, path: str, output_path: str, window: int = 10\n",
    "    ) -> Tuple[List[int], List[str]]:\n",
    "        is_silence = []\n",
    "        file_names = []\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        duration = librosa.get_duration(path=path)\n",
    "        song = AudioSegment.from_file(path, format=\"mp3\")\n",
    "        window_ms = 1000 * window\n",
    "        for j in range(1, math.floor(duration / window) + 1):\n",
    "            start_ = (j - 1) * window_ms\n",
    "            end_ = j * window_ms\n",
    "            seg_ = song[start_:end_]\n",
    "            seg_name = f\"sec_{int(start_ / 1000)}_{int(end_ / 1000)}.mp3\"\n",
    "            seg_.export(\n",
    "                os.path.join(output_path, seg_name),\n",
    "                format=\"mp3\",\n",
    "            )\n",
    "            file_names.append(seg_name)\n",
    "            # Is it silence?\n",
    "            dBFS = seg_.dBFS\n",
    "            silence_results = silence.detect_silence(\n",
    "                seg_, min_silence_len=1000, silence_thresh=dBFS - 16\n",
    "            )\n",
    "            total_silence = [(stop - start) / 1000 for start, stop in silence_results]\n",
    "            total_silence = np.sum(total_silence)\n",
    "            if total_silence >= 0.5 * window:\n",
    "                is_silence.append(1)\n",
    "            else:\n",
    "                is_silence.append(0)\n",
    "        return is_silence, file_names\n",
    "\n",
    "    def _feature_extraction(self, directory: str) -> Tuple[np.ndarray, List[str]]:\n",
    "        f1, _, feature_names = mtf.directory_feature_extraction(\n",
    "            directory, 1, 1, 0.1, 0.1\n",
    "        )\n",
    "        mid_term_features = [f1]\n",
    "        # convert list of feature matrices to x, y format:\n",
    "        x, y = at.features_to_matrix(mid_term_features)\n",
    "        m = x.mean(axis=0)\n",
    "        s = np.std(x, axis=0)\n",
    "        X = (x - m) / s\n",
    "        return X, feature_names\n",
    "\n",
    "    def _feature_selection(\n",
    "        self, X: np.ndarray, feature_names: List[str]\n",
    "    ) -> Tuple[np.ndarray, List[str]]:\n",
    "        # Choose Features that have some variability\n",
    "        threshold = 1\n",
    "        selector = VarianceThreshold(threshold=threshold)\n",
    "        X_selected = selector.fit_transform(X)\n",
    "        selected_feature_indices = selector.get_support(indices=True)\n",
    "        selected_feature_names = [feature_names[i] for i in selected_feature_indices]\n",
    "        return X_selected, selected_feature_names\n",
    "\n",
    "    def _outlier_detection(self, X: np.ndarray, num_high: int):\n",
    "        clf = LocalOutlierFactor(n_neighbors=20, metric=\"cosine\")\n",
    "        clf.fit(X)\n",
    "        outlier_scores = clf.negative_outlier_factor_\n",
    "        sorted_indices = np.argsort(outlier_scores)\n",
    "        highlight_indices = sorted_indices[:num_high]\n",
    "        # print(outlier_detection(X_new, 10))\n",
    "        return highlight_indices\n",
    "\n",
    "    def get_distances(\n",
    "        self,\n",
    "        video_path: str,\n",
    "        temp_path: str,\n",
    "        window: int = 10,\n",
    "    ) -> np.ndarray:\n",
    "        temp_file_path = os.path.join(temp_path, \"audio\")\n",
    "        # Load the Video\n",
    "        video = editor.VideoFileClip(video_path)\n",
    "        # Extract the Audio\n",
    "        audio = video.audio\n",
    "        # Export the Audio\n",
    "        audio_path = os.path.join(temp_file_path, \"all_audio\")\n",
    "        os.makedirs(audio_path, exist_ok=True)\n",
    "        audio.write_audiofile(os.path.join(audio_path, \"audio.mp3\"))\n",
    "        del video, audio\n",
    "\n",
    "        is_silence, file_names = self._audio_seg(\n",
    "            path=os.path.join(audio_path, \"audio.mp3\"),\n",
    "            output_path=temp_file_path,\n",
    "            window=window,\n",
    "        )\n",
    "        # Make sure that we sort the results in the same way as they will get processed.\n",
    "        is_silence = [x for _, x in sorted(zip(file_names, is_silence), key=lambda pair: pair[0])]\n",
    "\n",
    "        X, feature_names = self._feature_extraction(directory=temp_file_path)\n",
    "        X, feature_names = self._feature_selection(X=X, feature_names=feature_names)\n",
    "\n",
    "        distances = cosine_distances(X, X)\n",
    "        del X\n",
    "        median_distances = np.median(distances, axis=1)\n",
    "        # Make distance equal to 0 for the silences to be ignored.\n",
    "        median_distances = np.where(np.array(is_silence) == 1, 0, median_distances)\n",
    "\n",
    "        # highlights = self._outlier_detection(X=X, num_high=num_highlights)\n",
    "\n",
    "        shutil.rmtree(temp_file_path, ignore_errors=True)\n",
    "        return median_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighlightsFinder:\n",
    "    def __init__(self, batch_size: int = 32) -> None:\n",
    "        self.ahf = AudioHighlightsFinder()\n",
    "        self.ihf = ImageHighlightsFinder(batch_size=batch_size)\n",
    "    \n",
    "    def _str_to_int_tuple(self, s: str) -> Tuple[int, int]:\n",
    "        start, end = s.split(\"_\")\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        return start, end\n",
    "\n",
    "    def _merge_timestamps(self, timestamps: List[Tuple]) -> List[Tuple]:\n",
    "        merged_timestamps = []\n",
    "        timestamps.sort(key=lambda x: x[0])  # Sort the timestamps based on start time\n",
    "\n",
    "        for timestamp in timestamps:\n",
    "            if merged_timestamps and timestamp[0] == merged_timestamps[-1][1]:\n",
    "                merged_timestamps[-1] = (\n",
    "                    merged_timestamps[-1][0],\n",
    "                    timestamp[1],\n",
    "                )  # Extend the previous timestamp\n",
    "            else:\n",
    "                merged_timestamps.append(timestamp)  # Add a new timestamp\n",
    "\n",
    "        return merged_timestamps\n",
    "\n",
    "    def _convert_str_to_timestamps(self, highlights: List[str]) -> List[Tuple]:\n",
    "        timestamps = [\n",
    "            self._str_to_int_tuple(s=timestamp)\n",
    "            for timestamp in highlights\n",
    "        ]\n",
    "        timestamps = self._merge_timestamps(timestamps=timestamps)\n",
    "        return timestamps\n",
    "\n",
    "    def create_video_summary(\n",
    "        self, video_path: str, summary_output: str, num_highlights: int, window: int\n",
    "    ):\n",
    "        temp_file_dir = \"temp\"\n",
    "        temp_file_path = os.path.join(temp_file_dir, str(uuid4()))\n",
    "\n",
    "        # Get the distances from each modality.\n",
    "        image_distances = self.ihf.get_distances(\n",
    "            video_path=video_path, temp_path=temp_file_path, window=window\n",
    "        )\n",
    "        audio_distances = self.ahf.get_distances(\n",
    "            video_path=video_path, temp_path=temp_file_path, window=window\n",
    "        )\n",
    "        assert image_distances.shape[0] == audio_distances.shape[0]\n",
    "\n",
    "        # TODO: Add weight to each modality.\n",
    "        distances = np.add(image_distances, audio_distances)\n",
    "\n",
    "        # Get the idx of the segments with the greater distance.\n",
    "        idx = np.argsort(distances)[-num_highlights:]\n",
    "\n",
    "        # Load the video.\n",
    "        video = editor.VideoFileClip(video_path)\n",
    "        # Get the duration of the video in secs.\n",
    "        duration = video.duration\n",
    "\n",
    "        # Create the timestamps in the same way as they will get processed.\n",
    "        timestamps = [\n",
    "            f\"{(j - 1) * window}_{j * window}\"\n",
    "            for j in range(1, math.floor(duration / window) + 1)\n",
    "        ]\n",
    "        timestamps = list(sorted(timestamps))\n",
    "\n",
    "        # # Get the timestamps of the segments with the greater distance.\n",
    "        highlights = np.array(timestamps)[idx].tolist()\n",
    "\n",
    "        timestamps = self._convert_str_to_timestamps(highlights=highlights)\n",
    "        \n",
    "        # Create the summary video.\n",
    "        clips = []\n",
    "        for start_time , end_time in timestamps:\n",
    "            clip = video.subclip(start_time, end_time)\n",
    "            clips.append(clip)\n",
    "        \n",
    "        final = editor.concatenate_videoclips(clips)\n",
    "        \n",
    "        # Get video's name.\n",
    "        video_name = os.path.basename(video_path)\n",
    "        video_name = Path(video_name).stem\n",
    "        os.makedirs(summary_output, exist_ok=True)\n",
    "        final.write_videofile(os.path.join(summary_output, f\"{video_name}_summary.mp4\"))\n",
    "        # Delete the temp dir.\n",
    "        shutil.rmtree(temp_file_dir, ignore_errors=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Download Videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=SvV6aUki6LU&list=PLCGIzmTE4d0iCqSmha1X7F-_AqB3jjo26&index=7&ab_channel=FIFA\n",
      "[youtube:tab] Downloading just the video SvV6aUki6LU because of --no-playlist\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=SvV6aUki6LU\n",
      "[youtube] SvV6aUki6LU: Downloading webpage\n",
      "[youtube] SvV6aUki6LU: Downloading ios player API JSON\n",
      "[youtube] SvV6aUki6LU: Downloading android player API JSON\n",
      "[youtube] SvV6aUki6LU: Downloading m3u8 information\n",
      "[info] SvV6aUki6LU: Downloading 1 format(s): 614+140\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 1207\n",
      "[download] Destination: data\\SvV6aUki6LU.f614.mp4\n",
      "[download] 100% of    1.70GiB in 00:04:57 at 5.84MiB/s                        \n",
      "[download] Destination: data\\SvV6aUki6LU.f140.m4a\n",
      "[download] 100% of   94.38MiB in 00:00:08 at 10.59MiB/s    \n",
      "[Merger] Merging formats into \"data\\SvV6aUki6LU.mp4\"\n",
      "Deleting original file data\\SvV6aUki6LU.f140.m4a (pass -k to keep)\n",
      "Deleting original file data\\SvV6aUki6LU.f614.mp4 (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "links = [\n",
    "    # \"https://www.youtube.com/watch?v=d0r0vzvqeoc&ab_channel=LubenTV\",\n",
    "    \"https://www.youtube.com/watch?v=SvV6aUki6LU&list=PLCGIzmTE4d0iCqSmha1X7F-_AqB3jjo26&index=7&ab_channel=FIFA\",\n",
    "]\n",
    "\n",
    "ydl_opts = {\"noplaylist\": True, \"outtmpl\": os.path.join(save_videos_to, \"%(id)s\"), \"format\": \"bv*[ext=mp4]+ba[ext=m4a]/b[ext=mp4] / bv*+ba/b\"}\n",
    "\n",
    "with YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download(links)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create Video Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = HighlightsFinder(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_output_path = [f for f in glob.glob(os.path.join(save_videos_to, \"*.mp4\")) if \"_summary\" not in f and \"ipynb_checkpoints\" not in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_path in videos_output_path[1:]:\n",
    "    hf.create_video_summary(video_path=video_path, summary_output=summary_output, num_highlights=num_highlights, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videosummarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
