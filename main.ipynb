{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import av\n",
    "import numpy as np\n",
    "import torch\n",
    "from moviepy import editor\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from transformers import (\n",
    "    VideoMAEForVideoClassification,\n",
    "    VideoMAEImageProcessor,\n",
    "    VideoMAEModel,\n",
    ")\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got them from: https://huggingface.co/docs/transformers/model_doc/videomae#transformers.VideoMAEForVideoClassification.forward.example\n",
    "\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    \"\"\"\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_video_to_clips(video_path: str, output_folder: str, window: int) -> None:\n",
    "    # Load video and get its duration.\n",
    "    clip = editor.VideoFileClip(video_path)\n",
    "    duration = clip.duration\n",
    "    # del clip\n",
    "\n",
    "    start_ = 0\n",
    "    end_ = window\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    while end_ < duration:\n",
    "        # new_clip = clip.subclip(start_, end_)\n",
    "        # new_clip.write_videofile(os.path.join(output_folder, f\"{start_}_{end_}.mp4\"), threads=1, preset='ultrafast')\n",
    "\n",
    "        ffmpeg_extract_subclip(\n",
    "            video_path,\n",
    "            start_,\n",
    "            end_,\n",
    "            targetname=os.path.join(output_folder, f\"{start_}_{end_}.mp4\"),\n",
    "        )\n",
    "        start_ += window\n",
    "        end_ += window  # TODO: There is a possibility to miss the last clip. Maybe we should set an upper bound."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Split a video into clips**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"test_video.mp4\"\n",
    "clips_output_path = os.path.basename(video_path)\n",
    "clips_output_path = Path(clips_output_path).stem\n",
    "\n",
    "split_video_to_clips(video_path=video_path, output_folder=clips_output_path, window=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extract audio from a clip**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#Load the Video\n",
    "video = editor.VideoFileClip(\"test_video.mp4\")\n",
    "\n",
    "#Extract the Audio\n",
    "audio = video.audio\n",
    "\n",
    "#Export the Audio\n",
    "audio.write_audiofile(\"audio.mp3\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extract a feature vector from a video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<av.InputContainer 'test_video.mp4'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av.open(\"test_video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_clips(clips_output_path: str) -> List[List]:\n",
    "    file_paths = glob.glob(os.path.join(clips_output_path, \"*\"))\n",
    "    clips = [av.open(file_path) for file_path in file_paths]\n",
    "\n",
    "    # sample 16 frames\n",
    "    indices = [sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=clip.streams.video[0].frames) for clip in clips]\n",
    "    videos = [list(read_video_pyav(clip, indices_)) for clip, indices_ in zip(clips, indices)]\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics were not used when initializing VideoMAEModel: ['fc_norm.weight', 'classifier.bias', 'fc_norm.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing VideoMAEModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VideoMAEModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "processor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n",
    "# model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n",
    "model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = open_clips(clips_output_path=clips_output_path)\n",
    "\n",
    "# prepare videos for the model\n",
    "inputs = processor(videos, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    # logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1568, 768])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(videos) > 1:\n",
    "    video_features = outputs.last_hidden_state[:, 0, :]\n",
    "else:\n",
    "    video_features = outputs.last_hidden_state.squeeze(dim=0)[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 768])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videosummarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
