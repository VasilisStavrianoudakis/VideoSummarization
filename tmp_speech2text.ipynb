{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "\n",
    "# warnings.simplefilter(action='ignore')\n",
    "import os\n",
    "import shutil\n",
    "# import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Optional, Tuple, Union\n",
    "from uuid import uuid4\n",
    "\n",
    "# import av\n",
    "import cv2\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from moviepy import editor\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "from PIL import Image\n",
    "from pyAudioAnalysis import MidTermFeatures as mtf\n",
    "from pyAudioAnalysis import audioTrainTest as at\n",
    "from pydub import AudioSegment\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from torchvision import models, transforms\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "# from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_videos_to = \"data\"\n",
    "window = 10\n",
    "summary_output = \"videos_summary\"\n",
    "num_highlights = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageHighlightsFinder:\n",
    "    def __init__(self, batch_size: int = 32) -> None:\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.batch_size = batch_size\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        self.model = torch.nn.Sequential(*(list(model.children())[:-1])).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def _get_transformations(self, will_be_saved: bool) -> List[Any]:\n",
    "        transformations = [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "        if will_be_saved:\n",
    "            transformations.append(transforms.ToPILImage())\n",
    "        return transformations\n",
    "\n",
    "    def _preprocess_image(\n",
    "        self, image: Image.Image, will_be_saved: bool = False\n",
    "    ) -> Union[torch.Tensor, Image.Image]:\n",
    "        transformations = self._get_transformations(will_be_saved=will_be_saved)\n",
    "\n",
    "        transform = transforms.Compose(transformations)\n",
    "        image = transform(image)\n",
    "        if will_be_saved:\n",
    "            return image\n",
    "\n",
    "        image = image.unsqueeze(0)\n",
    "        # print(image.shape)\n",
    "        return image\n",
    "\n",
    "    def _chunks(self, lst, n):\n",
    "        \"\"\"\n",
    "        Yield successive n-sized chunks from lst.\n",
    "        \"\"\"\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i : i + n]\n",
    "\n",
    "    def _create_feature_vectors(self, file_paths: List[str]) -> np.ndarray:\n",
    "        features = None\n",
    "        for file_paths_chunk in self._chunks(file_paths, n=self.batch_size):\n",
    "            # Get the data for this batch.\n",
    "            imgs = [Image.open(img).convert(\"RGB\") for img in file_paths_chunk]\n",
    "            imgs = [self._preprocess_image(img) for img in imgs]\n",
    "            imgs = torch.cat(imgs, dim=0).to(self.device)\n",
    "\n",
    "            # Convert them to features.\n",
    "            with torch.no_grad():\n",
    "                f = self.model(imgs)\n",
    "\n",
    "            if features is None:\n",
    "                features = f.clone()\n",
    "            else:\n",
    "                features = torch.cat((features, f), dim=0)\n",
    "\n",
    "        features = features.squeeze(dim=-1)\n",
    "        features = features.squeeze(dim=-1)\n",
    "        features = features.cpu().detach().numpy()\n",
    "        return features\n",
    "\n",
    "    def _extract_frames(\n",
    "        self,\n",
    "        video_path: str,\n",
    "        images_folder: str,\n",
    "        start_at_sec: int = 5,\n",
    "        window: int = 10,\n",
    "    ):\n",
    "        os.makedirs(images_folder, exist_ok=True)\n",
    "        vidcap = cv2.VideoCapture(video_path)\n",
    "        success, image = vidcap.read()\n",
    "        success = True\n",
    "        while success:\n",
    "            vidcap.set(\n",
    "                cv2.CAP_PROP_POS_MSEC, (start_at_sec * 1000)\n",
    "            )  # One frame per second.\n",
    "            success, image = vidcap.read()\n",
    "            # print(\"Read a new frame: \", success)\n",
    "            if success:\n",
    "                cv2.imwrite(\n",
    "                    os.path.join(images_folder, f\"sec_{start_at_sec}.jpg\"), image\n",
    "                )  # save frame as JPEG file.\n",
    "            start_at_sec += window\n",
    "\n",
    "    def get_distances(\n",
    "        self,\n",
    "        video_path: str,\n",
    "        temp_path: str,\n",
    "        window: int = 10,\n",
    "    ) -> np.ndarray:\n",
    "        temp_file_path = os.path.join(temp_path, \"image\")\n",
    "        start_at_sec = int(window * 0.5)\n",
    "        self._extract_frames(\n",
    "            video_path=video_path,\n",
    "            images_folder=temp_file_path,\n",
    "            start_at_sec=start_at_sec,\n",
    "            window=window,\n",
    "        )\n",
    "        file_paths = glob.glob(os.path.join(temp_file_path, \"*.jpg\"))\n",
    "        features = self._create_feature_vectors(file_paths=file_paths)\n",
    "        distances = cosine_distances(features, features)\n",
    "        del features\n",
    "        median_distances = np.median(distances, axis=1)\n",
    "        del distances\n",
    "        assert median_distances.shape[0] == len(file_paths)\n",
    "        shutil.rmtree(temp_file_path, ignore_errors=True)\n",
    "        return median_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VideoFileClip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m VideoFileClip\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VideoFileClip' is not defined"
     ]
    }
   ],
   "source": [
    "VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./data/SvV6aUki6LU.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "video_file = './data/SvV6aUki6LU.mp4'\n",
    "filename, ext = os.path.splitext(video_file)\n",
    "clip = VideoFileClip(video_file)\n",
    "clip.audio.write_audiofile(f\"{filename}.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/SvV6aUki6LU', '.mp4')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename, ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "\n",
    "def convert_video_to_audio_moviepy(video_file, output_ext=\"mp3\"):\n",
    "\n",
    "    filename, ext = os.path.splitext(video_file)\n",
    "    clip = VideoFileClip(video_file)\n",
    "    clip.audio.write_audiofile(f\"{filename}.{output_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename, ext = os.path.splitext(video_file)\n",
    "    subprocess.call([\"ffmpeg\", \"-y\", \"-i\", video_file, f\"{filename}.{output_ext}\"], \n",
    "                    stdout=subprocess.DEVNULL,\n",
    "                    stderr=subprocess.STDOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _audio_seg(path: str, output_path: str, window: int = 10):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    duration = librosa.get_duration(path=path)\n",
    "    song = AudioSegment.from_file(path, format=\"mp3\")\n",
    "    window_ms = 1000 * window\n",
    "    for j in range(1, math.floor(duration / window) + 1):\n",
    "        start_ = (j - 1) * window_ms\n",
    "        end_ = j * window_ms\n",
    "        seg_ = song[start_:end_]\n",
    "        seg_.export(\n",
    "            os.path.join(\n",
    "                output_path, f\"sec_{int(start_ / 1000)}_{int(end_ / 1000)}.mp3\"\n",
    "            ),\n",
    "            format=\"mp3\",\n",
    "        )\n",
    "\n",
    "_audio_seg('./data/SvV6aUki6LU.mp4', './data/audio/', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioHighlightsFinder:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def _audio_seg(self, path: str, output_path: str, window: int = 10):\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        duration = librosa.get_duration(path=path)\n",
    "        song = AudioSegment.from_file(path, format=\"mp3\")\n",
    "        window_ms = 1000 * window\n",
    "        for j in range(1, math.floor(duration / window) + 1):\n",
    "            start_ = (j - 1) * window_ms\n",
    "            end_ = j * window_ms\n",
    "            seg_ = song[start_:end_]\n",
    "            seg_.export(\n",
    "                os.path.join(\n",
    "                    output_path, f\"sec_{int(start_ / 1000)}_{int(end_ / 1000)}.mp3\"\n",
    "                ),\n",
    "                format=\"mp3\",\n",
    "            )\n",
    "\n",
    "    def _feature_extraction(self, directory: str) -> Tuple[np.ndarray, List[str]]:\n",
    "        f1, _, feature_names = mtf.directory_feature_extraction(\n",
    "            directory, 1, 1, 0.1, 0.1\n",
    "        )\n",
    "        mid_term_features = [f1]\n",
    "        # convert list of feature matrices to x, y format:\n",
    "        x, y = at.features_to_matrix(mid_term_features)\n",
    "        m = x.mean(axis=0)\n",
    "        s = np.std(x, axis=0)\n",
    "        X = (x - m) / s\n",
    "        return X, feature_names\n",
    "\n",
    "    def _feature_selection(\n",
    "        self, X: np.ndarray, feature_names: List[str]\n",
    "    ) -> Tuple[np.ndarray, List[str]]:\n",
    "        # Choose Features that have some variability\n",
    "        threshold = 1\n",
    "        selector = VarianceThreshold(threshold=threshold)\n",
    "        X_selected = selector.fit_transform(X)\n",
    "        selected_feature_indices = selector.get_support(indices=True)\n",
    "        selected_feature_names = [feature_names[i] for i in selected_feature_indices]\n",
    "        return X_selected, selected_feature_names\n",
    "\n",
    "    def _outlier_detection(self, X: np.ndarray, num_high: int):\n",
    "        clf = LocalOutlierFactor(n_neighbors=20, metric=\"cosine\")\n",
    "        clf.fit(X)\n",
    "        outlier_scores = clf.negative_outlier_factor_\n",
    "        sorted_indices = np.argsort(outlier_scores)\n",
    "        highlight_indices = sorted_indices[:num_high]\n",
    "        # print(outlier_detection(X_new, 10))\n",
    "        return highlight_indices\n",
    "\n",
    "    def get_distances(\n",
    "        self,\n",
    "        video_path: str,\n",
    "        temp_path: str, \n",
    "        window: int = 10,\n",
    "    ) -> np.ndarray:\n",
    "        temp_file_path = os.path.join(temp_path, \"audio\")\n",
    "        # Load the Video\n",
    "        video = editor.VideoFileClip(video_path)\n",
    "        # Extract the Audio\n",
    "        audio = video.audio\n",
    "        # Export the Audio\n",
    "        audio_path = os.path.join(temp_file_path, \"all_audio\")\n",
    "        os.makedirs(audio_path, exist_ok=True)\n",
    "        audio.write_audiofile(os.path.join(audio_path, \"audio.mp3\"))\n",
    "        del video, audio\n",
    "\n",
    "        self._audio_seg(\n",
    "            path=os.path.join(audio_path, \"audio.mp3\"),\n",
    "            output_path=temp_file_path,\n",
    "            window=window,\n",
    "        )\n",
    "\n",
    "        X, feature_names = self._feature_extraction(directory=temp_file_path)\n",
    "        X, feature_names = self._feature_selection(X=X, feature_names=feature_names)\n",
    "\n",
    "        distances = cosine_distances(X, X)\n",
    "        del X\n",
    "        median_distances = np.median(distances, axis=1)\n",
    "\n",
    "        # highlights = self._outlier_detection(X=X, num_high=num_highlights)\n",
    "\n",
    "        # shutil.rmtree(temp_file_path, ignore_errors=True)\n",
    "        return median_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighlightsFinder:\n",
    "    def __init__(self, batch_size: int = 32) -> None:\n",
    "        self.ahf = AudioHighlightsFinder()\n",
    "        self.ihf = ImageHighlightsFinder(batch_size=batch_size)\n",
    "    \n",
    "    def _str_to_int_tuple(self, s: str) -> Tuple[int, int]:\n",
    "        start, end = s.split(\"_\")\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        return start, end\n",
    "\n",
    "    def _merge_timestamps(self, timestamps: List[Tuple]) -> List[Tuple]:\n",
    "        merged_timestamps = []\n",
    "        timestamps.sort(key=lambda x: x[0])  # Sort the timestamps based on start time\n",
    "\n",
    "        for timestamp in timestamps:\n",
    "            if merged_timestamps and timestamp[0] == merged_timestamps[-1][1]:\n",
    "                merged_timestamps[-1] = (\n",
    "                    merged_timestamps[-1][0],\n",
    "                    timestamp[1],\n",
    "                )  # Extend the previous timestamp\n",
    "            else:\n",
    "                merged_timestamps.append(timestamp)  # Add a new timestamp\n",
    "\n",
    "        return merged_timestamps\n",
    "\n",
    "    def _convert_str_to_timestamps(self, highlights: List[str]) -> List[Tuple]:\n",
    "        timestamps = [\n",
    "            self._str_to_int_tuple(s=timestamp)\n",
    "            for timestamp in highlights\n",
    "        ]\n",
    "        timestamps = self._merge_timestamps(timestamps=timestamps)\n",
    "        return timestamps\n",
    "\n",
    "    def create_video_summary(\n",
    "        self, video_path: str, summary_output: str, num_highlights: int, window: int\n",
    "    ):\n",
    "        temp_file_dir = \"temp\"\n",
    "        temp_file_path = os.path.join(temp_file_dir, str(uuid4()))\n",
    "\n",
    "        # Get the distances from each modality.\n",
    "        # image_distances = self.ihf.get_distances(\n",
    "        #     video_path=video_path, temp_path=temp_file_path, window=window\n",
    "        # )\n",
    "        audio_distances = self.ahf.get_distances(\n",
    "            video_path=video_path, temp_path=temp_file_path, window=window\n",
    "        )\n",
    "        # assert image_distances.shape[0] == audio_distances.shape[0]\n",
    "\n",
    "        # # TODO: Add weight to each modality.\n",
    "        # distances = np.add(image_distances, audio_distances)\n",
    "\n",
    "        # # Get the idx of the segments with the greater distance.\n",
    "        # idx = np.argsort(distances)[-num_highlights:]\n",
    "\n",
    "        # # Load the video.\n",
    "        # video = editor.VideoFileClip(video_path)\n",
    "        # # Get the duration of the video in secs.\n",
    "        # duration = video.duration\n",
    "\n",
    "        # # Create the timestamps in the same way as they will get processed.\n",
    "        # timestamps = [\n",
    "        #     f\"{(j - 1) * window}_{j * window}\"\n",
    "        #     for j in range(1, math.floor(duration / window) + 1)\n",
    "        # ]\n",
    "        # timestamps = list(sorted(timestamps))\n",
    "\n",
    "        # # # Get the timestamps of the segments with the greater distance.\n",
    "        # highlights = np.array(timestamps)[idx].tolist()\n",
    "\n",
    "        # timestamps = self._convert_str_to_timestamps(highlights=highlights)\n",
    "        \n",
    "        # # Create the summary video.\n",
    "        # clips = []\n",
    "        # for start_time , end_time in timestamps:\n",
    "        #     clip = video.subclip(start_time, end_time)\n",
    "        #     clips.append(clip)\n",
    "        \n",
    "        # final = editor.concatenate_videoclips(clips)\n",
    "        \n",
    "        # # Get video's name.\n",
    "        # video_name = os.path.basename(video_path)\n",
    "        # video_name = Path(video_name).stem\n",
    "        # os.makedirs(summary_output, exist_ok=True)\n",
    "        # final.write_videofile(os.path.join(summary_output, f\"{video_name}_summary.mp4\"))\n",
    "        # # Delete the temp dir.\n",
    "        # shutil.rmtree(temp_file_dir, ignore_errors=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Download Videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links = [\n",
    "#     # \"https://www.youtube.com/watch?v=d0r0vzvqeoc&ab_channel=LubenTV\",\n",
    "#     \"https://www.youtube.com/watch?v=SvV6aUki6LU&list=PLCGIzmTE4d0iCqSmha1X7F-_AqB3jjo26&index=7&ab_channel=FIFA\",\n",
    "# ]\n",
    "\n",
    "# ydl_opts = {\"noplaylist\": True, \"outtmpl\": os.path.join(save_videos_to, \"%(id)s\"), \"format\": \"bv*[ext=mp4]+ba[ext=m4a]/b[ext=mp4] / bv*+ba/b\"}\n",
    "\n",
    "# with YoutubeDL(ydl_opts) as ydl:\n",
    "#     ydl.download(links)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create Video Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = HighlightsFinder(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_output_path = [f for f in glob.glob(os.path.join(save_videos_to, \"*.mp4\")) if \"_summary\" not in f and \"ipynb_checkpoints\" not in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\SvV6aUki6LU.mp4']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_path in videos_output_path:\n",
    "    hf.create_video_summary(video_path=video_path, summary_output=summary_output, num_highlights=num_highlights, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing file 1 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_1030_1040.mp3\n",
      "Analyzing file 2 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_1130_1140.mp3\n",
      "Analyzing file 3 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_1200_1210.mp3\n",
      "Analyzing file 4 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_130_140.mp3\n",
      "Analyzing file 5 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_1340_1350.mp3\n",
      "Analyzing file 6 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_1440_1450.mp3\n",
      "Analyzing file 7 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_1510_1520.mp3\n",
      "Analyzing file 8 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_1610_1620.mp3\n",
      "Analyzing file 9 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_1730_1740.mp3\n",
      "Analyzing file 10 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_1740_1750.mp3\n",
      "Analyzing file 11 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_1920_1930.mp3\n",
      "Analyzing file 12 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_200_210.mp3\n",
      "Analyzing file 13 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_2080_2090.mp3\n",
      "Analyzing file 14 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_2350_2360.mp3\n",
      "   (EMPTY FILE -- SKIPPING)\n",
      "Analyzing file 15 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_270_280.mp3\n",
      "Analyzing file 16 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_370_380.mp3\n",
      "Analyzing file 17 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_50_60.mp3\n",
      "Analyzing file 18 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_530_540.mp3\n",
      "Analyzing file 19 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_690_700.mp3\n",
      "Analyzing file 20 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_840_850.mp3\n",
      "Analyzing file 21 of 21: ./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio\\sec_930_940.mp3\n",
      "Feature extraction complexity ratio: 24.0 x realtime\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X, feature_names = hf.ahf._feature_extraction(directory='./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio/')\n",
    "X, feature_names = hf.ahf._feature_selection(X=X, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:55<00:00, 2.64MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "model_whisper = whisper.load_model(\"base.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_whisper.transcribe('./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio/sec_1080_1090.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \" Flicking by Greece but I'm flicked on and frauds take the lead in the World Cup final\",\n",
       " 'segments': [{'id': 0,\n",
       "   'seek': 0,\n",
       "   'start': 0.0,\n",
       "   'end': 6.12,\n",
       "   'text': \" Flicking by Greece but I'm flicked on and frauds take the lead in the World Cup\",\n",
       "   'tokens': [50363,\n",
       "    1610,\n",
       "    7958,\n",
       "    416,\n",
       "    10315,\n",
       "    475,\n",
       "    314,\n",
       "    1101,\n",
       "    781,\n",
       "    9484,\n",
       "    319,\n",
       "    290,\n",
       "    7394,\n",
       "    82,\n",
       "    1011,\n",
       "    262,\n",
       "    1085,\n",
       "    287,\n",
       "    262,\n",
       "    2159,\n",
       "    5454,\n",
       "    50669],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.5892892250647912,\n",
       "   'compression_ratio': 1.0493827160493827,\n",
       "   'no_speech_prob': 0.11426424980163574},\n",
       "  {'id': 1,\n",
       "   'seek': 0,\n",
       "   'start': 6.12,\n",
       "   'end': 8.68,\n",
       "   'text': ' final',\n",
       "   'tokens': [50669, 2457, 50797],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.5892892250647912,\n",
       "   'compression_ratio': 1.0493827160493827,\n",
       "   'no_speech_prob': 0.11426424980163574}],\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1.42G/1.42G [05:16<00:00, 4.82MiB/s]\n"
     ]
    }
   ],
   "source": [
    "med_model_whisper = whisper.load_model(\"medium.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Flicked in by Griezmann and flicked on! And France take the lead in the World Cup Final!',\n",
       " 'segments': [{'id': 0,\n",
       "   'seek': 0,\n",
       "   'start': 0.0,\n",
       "   'end': 3.0,\n",
       "   'text': ' Flicked in by Griezmann and flicked on!',\n",
       "   'tokens': [50363,\n",
       "    1610,\n",
       "    9484,\n",
       "    287,\n",
       "    416,\n",
       "    402,\n",
       "    5034,\n",
       "    89,\n",
       "    9038,\n",
       "    290,\n",
       "    781,\n",
       "    9484,\n",
       "    319,\n",
       "    0,\n",
       "    50513],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.3637124094469794,\n",
       "   'compression_ratio': 1.0731707317073171,\n",
       "   'no_speech_prob': 0.12039558589458466},\n",
       "  {'id': 1,\n",
       "   'seek': 0,\n",
       "   'start': 3.0,\n",
       "   'end': 7.0,\n",
       "   'text': ' And France take the lead in the World Cup Final!',\n",
       "   'tokens': [50513,\n",
       "    843,\n",
       "    4881,\n",
       "    1011,\n",
       "    262,\n",
       "    1085,\n",
       "    287,\n",
       "    262,\n",
       "    2159,\n",
       "    5454,\n",
       "    8125,\n",
       "    0,\n",
       "    50713],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.3637124094469794,\n",
       "   'compression_ratio': 1.0731707317073171,\n",
       "   'no_speech_prob': 0.12039558589458466}],\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_model_whisper.transcribe('./temp/51a74d3a-907c-45a7-91cc-62a6a6acd1f1/audio/sec_1080_1090.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrclex import NRCLex  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "# Mention the installed location of Tesseract-OCR in your system\n",
    "pytesseract.pytesseract.tesseract_cmd = \"C:/Program Files/Tesseract-OCR/tesseract.exe\"\n",
    "\n",
    "# Read image from which text needs to be extracted\n",
    "img = cv2.imread(\"data\\SvV6aUki6LU\\sec_5975.jpg\")\n",
    "\n",
    "# Preprocessing the image starts\n",
    "\n",
    "# Convert the image to gray scale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Performing OTSU threshold\n",
    "ret, thresh1 = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# Specify structure shape and kernel size.\n",
    "# Kernel size increases or decreases the area\n",
    "# of the rectangle to be detected.\n",
    "rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 10))\n",
    "\n",
    "# Applying dilation on the threshold image\n",
    "dilation = cv2.dilate(thresh1, rect_kernel, iterations = 1)\n",
    "\n",
    "# Finding contours\n",
    "contours, hierarchy = cv2.findContours(dilation, cv2.RETR_EXTERNAL,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tcv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "# Creating a copy of image\n",
    "im2 = img.copy()\n",
    "\n",
    "# A text file is created and flushed\n",
    "file = open(\"recognized.txt\", \"w+\")\n",
    "file.write(\"\")\n",
    "file.close()\n",
    "\n",
    "# Looping through the identified contours\n",
    "# Then rectangular part is cropped and passed on\n",
    "# to pytesseract for extracting text from it\n",
    "# Extracted text is then written into the text file\n",
    "for cnt in contours:\n",
    "\tx, y, w, h = cv2.boundingRect(cnt)\n",
    "\t\n",
    "\t# Drawing a rectangle on copied image\n",
    "\trect = cv2.rectangle(im2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\t\n",
    "\t# Cropping the text block for giving input to OCR\n",
    "\tcropped = im2[y:y + h, x:x + w]\n",
    "\t\n",
    "\t# Open the file in append mode\n",
    "\tfile = open(\"recognized.txt\", \"a\")\n",
    "\t\n",
    "\t# Apply OCR on the cropped image\n",
    "\ttext = pytesseract.image_to_string(cropped)\n",
    "\t\n",
    "\t# Appending the text into file\n",
    "\tfile.write(text)\n",
    "\tfile.write(\"\\n\")\n",
    "\t\n",
    "\t# Close the file\n",
    "\tfile.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videosummarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
